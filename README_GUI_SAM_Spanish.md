En este archivo, describiré el uso de la interfaz gráfica de usuario creada, su objetivo principal, usos y limitaciones más destacadas, así como las posibles áreas de mejora a simple vista.

*Objetivo General y Frameworks Utilizados*

El objetivo principal del código creado es etiquetar de forma fácil y asistida conjuntos de datos en el formato YOLO de forma offline. Esto se debe a que, durante las competencias de robots, no siempre se dispone de una conexión estable a internet para utilizar servicios con esta capacidad. Para lograr esta tarea, se empleó el modelo Segment Anything Model proporcionado por Meta, el cual es de código libre y cuenta con toda la documentación necesaria en su página de GitHub. También se utilizaron otros frameworks populares de Python para procesamiento de imágenes, como OpenCV, PIL y Tkinter para la creación general de la interfaz gráfica. Cada función creada cuenta con una explicación detallada dentro del mismo código para un mayor entendimiento.

*Explicación General del Código*

En su función principal, el código accede principalmente al predictor de máscaras de SAM para hacer la predicción de la máscara con mayor porcentaje de interés dentro del área seleccionada. El área seleccionada puede ser un bounding box dibujado con el clic izquierdo del mouse o un simple punto seleccionado con un clic derecho. Al hacer esto, la imagen se actualiza mostrando la máscara resultante del proceso de segmentación realizado por SAM. La GUI cuenta con dos principales métodos de anotación compatibles entre sí: anotaciones manuales (recomendadas cuando se trabaja en imágenes con muchos objetos a etiquetar en cada una de ellas) y auto segmentación asistida. Para acceder a la anotación manual, simplemente se debe seleccionar cada objeto de interés junto con su clase y presionar el botón "Save Mask", o bien realizar la segmentación y elegir la clase, y presionar el botón "Control Izquierdo" del teclado para guardar la máscara y anotación de YOLO en un archivo .txt. Para generar otra anotación, simplemente se debe repetir el proceso para el número de objetos de interés que haya en cada imagen. Finalmente, las anotaciones guardadas se mostrarán en el "label_left" de la app automáticamente, con la finalidad de mostrar las anotaciones hechas por cada imagen para una mayor referencia visual. La otra funcionalidad con la que cuenta la app es la auto segmentación asistida, la cual está diseñada para carpetas que cuentan con un solo objeto por imagen. Para acceder a esta, primero debe existir un bounding box ya dibujado con el clic izquierdo o un punto seleccionado con el clic derecho del mouse. Para acceder a la segmentación automática usando la información guardada en el bounding box, simplemente se debe presionar la "flecha de navegación arriba" del teclado, y se hará la segmentación de la imagen actual, pasando a la siguiente imagen y mostrando el resultado de la segmentación en el visualizador. Si el objeto está centrado en la misma posición de la imagen, basta con volver a presionar el mismo botón para realizar la segmentación en la siguiente imagen y así sucesivamente para las demás. Cuando no se esté conforme con el resultado de la segmentación automática y la máscara resultante mostrada en el visualizador no sea adecuada, simplemente basta con presionar la "flecha de navegación izquierda" para volver a la imagen anterior, corregir el dibujo del bounding box en la nueva área de interés, y volver a presionar el botón de navegación arriba para continuar con la segmentación autónoma. Cuando el resultado no sea de agrado, se repite este proceso nuevamente. Para acceder a la segmentación automática con el punto seleccionado por el usuario (clic derecho), se realiza el mismo proceso con la diferencia de que ahora se presiona el botón "Shift Izquierdo" en lugar de la "navegación arriba". Estas anotaciones, a diferencia de cuando se accede a la instancia de muchas anotaciones, no se guardan automáticamente en los archivos .txt creados, sino que se guardan en unos diccionarios especiales. Cuando se esté conforme con el resultado de todas estas anotaciones, simplemente se debe presionar el botón "Save Annotations" de la GUI, y las anotaciones realizadas quedarán escritas en los correspondientes .txt. Cuando se muestre en la terminal la leyenda "Anotaciones guardadas con éxito", indica que el proceso se ha completado correctamente. Para realizar auto segmentaciones de más objetos una vez terminada una carpeta actual, se debe cerrar la app y volver a abrirla seleccionando la nueva carpeta.

Instrucciones de Uso de la App

Al momento de ejecutar el código, lo primero que se debe hacer es elegir la carpeta que contiene las imágenes a segmentar. Una vez elegida la carpeta, se mostrarán las imágenes contenidas y se abrirá una ventana de creación de clases. Dentro de esta ventana, que cuenta con un área de escritura de texto, se pueden ingresar las clases una por una oprimiendo el botón "Confirm", o bien tenerlas todas preparadas en un archivo .txt, separadas por comas, y simplemente copiar y pegar ese mensaje y presionar el botón "Confirm" para que todas esas clases queden guardadas. Para visualizar si las clases quedaron guardadas de una forma satisfactoria, se puede minimizar la ventana de creación de clases y presionar el botón de la GUI donde estas se muestran. De haberse anotado todas satisfactoriamente, se mostrarán en la lista de clases posibles.

Como segundo punto, se pueden realizar los procesos de segmentación explicados en la sección "explicación del código". Una vez se finalice con las segmentaciones y anotaciones realizadas en la carpeta actual, la app puede cerrarse y volverse a abrir para elegir una carpeta diferente en la cual realizar anotaciones nuevas.

Finalmente, el manejo de archivos en la app es el siguiente: primero se toma el directorio padre, el cual se encuentra en el mismo fichero de la carpeta seleccionada. Dentro de ese directorio padre, se crea la carpeta "cache" (carpeta donde se guardan los resultados de los diversos procesos de segmentación) y la carpeta "data" (carpeta donde se forma la estructura del dataset en formato YOLO). Dentro de la carpeta "data" mencionada, se crean 3 subdirectorios: la carpeta "images" (donde se hacen copias de las imágenes trabajadas y segmentadas), la carpeta "labels" (donde se generan los .txt con las anotaciones poligonales de YOLO), y finalmente la carpeta "labels_bbox" (donde se generan los .txt con las anotaciones bounding box generadas). Como nota final, los procesos de segmentación múltiple, para imágenes que cuenten con muchos objetos en ellas y se realicen varias anotaciones para una sola imagen, solo se generan las anotaciones poligonales en esas instancias, mientras que en las anotaciones de auto segmentación se generan ambas (poligonales y bbox).

*Atajos Físicos Disponibles en la App*

Atajos del mouse:

    *Click Izquierdo del ratón: Posibilidad de dibujar un bounding box.
    *Click Derecho del ratón: Posibilidad de elegir un punto en la imagen al dar un clic.

Atajos del teclado:

    *Tecla de navegación derecha: Permite pasar hacia la siguiente imagen en la carpeta padre al presionarla.
    *Tecla de navegación izquierda: Permite pasar hacia la imagen anterior en la carpeta padre al presionarla.
    *Tecla Control Izquierdo: Permite guardar la máscara seleccionada en el proceso de segmentación (utilizable cuando se segmentan muchos objetos en una imagen).
    *Tecla de navegación arriba: Permite acceder a la función auto segment bbox (auto segmentación con información de bounding box).
    *Tecla Shift Derecho: Permite acceder a la función auto segment point (auto segmentación con información de las coordenadas del punto dibujado).

*Áreas de Mejora Posibles*

1.- Agregar una función que permita abrir otra carpeta diferente a la que se está trabajando actualmente, con la intención de que la app no se tenga que cerrar y evitar el proceso de volver a escribir las clases de la app (para hacer esto algunas variables como self.current_index y algunos contadores deberán reiniciarse).

2.- Agregar las anotaciones bbox también a los procesos de segmentación de muchos objetos en una imagen, dado que actualmente la lógica está descrita en otras funciones, pero en esa parte específica no está implementada.

3.- Mejorar la visualización de la app, elegir lugares más óptimos para los botones existentes dentro del frame de la GUI y agregar algunos sliders para funcionalidades futuras.

4.- Aplicar una instancia de división del dataset internamente. Actualmente, el dataset generado por la GUI como resultado se carga en Roboflow para su división en carpetas, lo cual no es tan óptimo si no se cuenta con conexión a internet. Sería oportuno aplicar una lógica interna para la división del mismo en el formato de YOLO y así tener también las carpetas de validación y test como output.

5.- Aplicar instancias internas de data augmentation (rotaciones de 90 y 180 grados principalmente). Actualmente se cuenta con la lógica de aumento de contraste, sin embargo, no está implementada en el flujo de la GUI. También se podría agregar un aumento de brillo. Para aplicar las rotaciones a las anotaciones en la carpeta cache, se guardan los resultados de los procesos de segmentación, se les podría aplicar rotaciones a estas imágenes y usar la lógica de generar anotaciones con la finalidad de generar esas anotaciones como una idea.

6.- Generar nombres únicos para las variables si esto causa problemas de sobreescritura. Para hacer esto, se puede aplicar timestamps o variables con nombres únicos.
